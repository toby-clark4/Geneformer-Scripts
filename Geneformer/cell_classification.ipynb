{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234afff3",
   "metadata": {},
   "source": [
    "## Geneformer Fine-Tuning for Cell Annotation Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cbe6178-ea4d-478a-80a8-65ffaa4c1820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "GPU_NUMBER = [0]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(s) for s in GPU_NUMBER])\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.chdir(\"/Users/clark04/Geneformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9885d9f-00ac-4c84-b6a3-b7b648a90f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from collections import Counter\n",
    "import torch\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns; sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from geneformer import DataCollatorForCellClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd3b98-5409-4105-b7af-f1ff64ea6a72",
   "metadata": {},
   "source": [
    "## Prepare training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5735f1b7-7595-4a02-be17-2c5b970ad81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cell type dataset (includes all tissues)\n",
    "train_dataset=load_from_disk(\"/Users/clark04/toby/Mesenchyme1_PSC1.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccb8110-e38f-42c4-b40e-849239a1be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21341\n",
      "['PSC1', 'Mesenchyme1']\n",
      "{'PSC1': 0, 'Mesenchyme1': 1}\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "evalset_list = []\n",
    "cell_list = []\n",
    "target_dict_list = []\n",
    "\n",
    "\n",
    "celltype_counter = Counter(train_dataset[\"cell_type\"])\n",
    "total_cells = sum(celltype_counter.values())\n",
    "print(total_cells)\n",
    "\n",
    "# shuffle datasets and rename columns\n",
    "trainset_cell_shuffled = train_dataset.shuffle(seed=42)\n",
    "trainset_cell_shuffled = trainset_cell_shuffled.rename_column(\"cell_type\",\"label\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_names = list(Counter(trainset_cell_shuffled[\"label\"]).keys())\n",
    "print(target_names)\n",
    "target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))\n",
    "print(target_name_id_dict)\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "labeled_trainset = trainset_cell_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "# create 80/20 train/eval splits\n",
    "labeled_train_split = labeled_trainset.select([i for i in range(0,round(len(labeled_trainset)*0.8))])\n",
    "labeled_eval_split = labeled_trainset.select([i for i in range(round(len(labeled_trainset)*0.8),len(labeled_trainset))])\n",
    "\n",
    "# filter dataset for cell types in corresponding training set\n",
    "trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "def if_trained_label(example):\n",
    "    return example[\"label\"] in trained_labels\n",
    "labeled_eval_split_subset = labeled_eval_split.filter(if_trained_label, num_proc=16)\n",
    "\n",
    "trainset = labeled_train_split\n",
    "evalset = labeled_eval_split_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb110d-ba43-4efc-bc43-1815d6912647",
   "metadata": {},
   "source": [
    "## Fine-Tune With Cell Classification Learning Objective and Quantify Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7b1cfb-f5cb-460e-ae77-769522ece054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaab7a4-cc13-4e8f-b137-ed18ff7b633c",
   "metadata": {},
   "source": [
    "### Please note that, as usual with deep learning models, we **highly** recommend tuning learning hyperparameters for all fine-tuning applications as this can significantly improve model performance. Example hyperparameters are defined below, but please see the \"hyperparam_optimiz_for_disease_classifier\" script for an example of how to tune hyperparameters for downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24e1ab7-0131-44bd-b458-1ce5ba31853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "# max input size\n",
    "max_input_size = 2 ** 11  # 2048\n",
    "\n",
    "# set training hyperparameters\n",
    "# max learning rate\n",
    "max_lr = 5e-5\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 2\n",
    "# number gpus\n",
    "num_gpus = 1\n",
    "# number cpu cores\n",
    "num_proc = 32\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 4\n",
    "# learning schedule\n",
    "lr_schedule_fn = \"linear\"\n",
    "# warmup steps\n",
    "warmup_steps = 500\n",
    "# number of epochs\n",
    "epochs = 4\n",
    "# optimizer\n",
    "optimizer = \"adamw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ce166ed-2d31-4b3c-b2bf-a1dae93019f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05164c24-5fbf-4372-b26c-a43f3777a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /mnt/scratchc/ghlab/toby/Geneformer/geneformer-12L-30M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/mnt/scratchc/ghlab/toby/Geneformer/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12808' max='17076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12808/17076 2:28:58 < 49:39, 1.43 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>0.023205</td>\n",
       "      <td>0.994377</td>\n",
       "      <td>0.994377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.069300</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.995080</td>\n",
       "      <td>0.995079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='1067' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 218/1067 00:47 < 03:07, 4.53 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cell_trainset = trainset\n",
    "cell_evalset = evalset\n",
    "cell_label_dict = target_name_id_dict\n",
    "\n",
    "# set logging steps\n",
    "logging_steps = round(len(cell_trainset)/geneformer_batch_size/10)\n",
    "\n",
    "# reload pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained(\"/mnt/scratchc/ghlab/toby/Geneformer/geneformer-12L-30M\", \n",
    "                                                  num_labels=2,\n",
    "                                                  output_attentions = False,\n",
    "                                                  output_hidden_states = False).to(\"cuda\")\n",
    "\n",
    "# define output directory path\n",
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "output_dir = f\"/mnt/scratchc/ghlab/toby/models/{datestamp}_geneformer_CellClassifier_Mesenchyme_PSC_L{max_input_size}_B{geneformer_batch_size}_LR{max_lr}_LS{lr_schedule_fn}_WU{warmup_steps}_E{epochs}_O{optimizer}_F{freeze_layers}/\"\n",
    "\n",
    "# ensure not overwriting previously saved model\n",
    "saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "if os.path.isfile(saved_model_test) == True:\n",
    "    raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "# make output directory\n",
    "subprocess.call(f'mkdir {output_dir}', shell=True)\n",
    "\n",
    "# set training arguments\n",
    "training_args = {\n",
    "    \"learning_rate\": max_lr,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"logging_steps\": logging_steps,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": lr_schedule_fn,\n",
    "    \"warmup_steps\": warmup_steps,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"output_dir\": output_dir,\n",
    "}\n",
    "\n",
    "training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "# create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_init,\n",
    "    data_collator=DataCollatorForCellClassification(),\n",
    "    train_dataset=cell_trainset,\n",
    "    eval_dataset=cell_evalset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# train the cell type classifier\n",
    "trainer.train()\n",
    "predictions = trainer.predict(cell_evalset)\n",
    "with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(predictions, fp)\n",
    "with torch.no_grad():\n",
    "    trainer.save_metrics(\"eval\",predictions.metrics)\n",
    "    trainer.save_model(output_dir)\n",
    "trainer = None\n",
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6950a1-6e71-4c4c-ad26-5e31ea51c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(classes_list, conf_mat, title, output_dir):\n",
    "    display_labels = []\n",
    "    i = 0\n",
    "    for label in classes_list:\n",
    "        display_labels += [\"{0}\\nn={1:.0f}\".format(label, sum(conf_mat[:,i]))]\n",
    "        i = i + 1\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix=preprocessing.normalize(conf_mat, norm=\"l1\"), \n",
    "                                     display_labels=display_labels)\n",
    "    display.plot(cmap=\"Blues\",values_format=\".3g\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f'{output_dir}/conf_mat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8a5d4-0f83-4eef-ac39-0fa28b611403",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predictions.predictions.argmax(axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "# Create a confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "classes_list = ['PSC1','Mesenchyme1']\n",
    "plot_confusion_matrix(classes_list, conf_matrix, 'Confusion Matrix for Cell Classifier of PSC1 vs Mesenchyme1',output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ce549-95af-48b7-8a23-072f12c81c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "eba1599a1f7e611c14c87ccff6793920aa63510b01fc0e229d6dd014149b8829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
